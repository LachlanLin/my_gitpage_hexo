---
title: 阅读SLAM相关论文系列----1
date: 2020-04-23 17:48:31
tags: 
- SLAM
- Deep Learning
mathjax: true
---
# 阅读SLAM相关论文(1)

## 1. DynaSLAM

[论文链接](https://arxiv.org/abs/1806.05620)

一个可以利用语义信息识别图像中物体是动态还是静态的SLAM.
SLAM的主体是ORB-SLAM2, 利用Mask R-CNN对语义信息进行提取, 同时利用多视图几何对物体的动态与否进行判断, 将两个方法的结果合并起来, 可以得到更好的效果, 将动态点作为离群点, 不考虑到定位与建图中, 论文中还进一步着重介绍他们的动态图像去除后恢复静态背景的功能, 不过暂时还不太成熟, 效果反而不如不恢复. 最后, 对该SLAM框架进行测试, 截止到论文发表为止, 在众多主流SLAM框架中, 取得了最好的效果.

### 1.1 总体框架

{% asset_img dynaslam-1.png '总体框架' %}

从图中可以看到, 输入的图像可以是单目的, 双目的, 或者RGB-D的, RGB图像经过Mask R-CNN, 直接生成了图像的掩膜, 掩膜区域内就是动态点. 另一边, 是使用多视图几何进行动态点的判断, 具体做法是: 选取关键帧中的5帧, 通过一个低成本的跟踪器, 计算当前帧的位姿, 然后计算某个特征点投影到当前帧的像素坐标和深度, 与该像素坐标上的当前深度作比较, 如果深度差超过一定阈值, 则判定为动态点, 同时周围一定范围内的等深度像素也都判定为动态点.
{% asset_img dynaslam-2.png '多视图几何计算深度示意图' %}

### 1.2 分割示例

{% asset_img dynaslam-3.png '多视图几何计算深度示示例' %}
图中, 第一幅图是使用多视图几何进行图像分割的, 可以可以看到, 该算法把最前面的人, 椅子和人手上拿着的书都分割出来了, 但是无法分割后面躲着的人; 第二幅图是使用Mask R-CNN进行语义分割的, 可以看到, 它把前面的人和后面的人都准确地分割出来了, 但是, 它无法分割出来人坐着的椅子和手上拿着的书; 第三幅图是把前两种算法的结果合并了, 可以看到, 该算法不仅把前后的人物给分割出来, 椅子和书也都分割出来了

### 1.3 效果对比

使用的数据集是TUM数据集

{% asset_img dynaslam-4.png '使用多视图和mrcnn的对比' %}
上表表示多种图像分割算法的对比
N表示使用多视图几何;
G表示使用mrcnn;
N+G表示使用多视图几何加mrcnn;
N+G+BI表示使用多视图几何, mrcnn和静态背景恢复;

{% asset_img dynaslam-5.png 'DynaSLAM和ORB-SLAM2的对比' %}
上表是RGB-D摄像头的DynaSLAM和ORB-SLAM2的效果对比, w代表场景走路walking, s代表场景坐下sitting. 可以看到在走路的高动态的场景中, DynaSLAM的效果比ORB-SLAM2的误差更小. 但是在坐下的低动态场景中, DynaSLAM的优势不怎么明显, 甚至比ORB-SLAM2还略低.

后缀意义如下:
(1) halfsphere: the camera moves following the trajectory of a 1-meter diameter half sphere,
(2) xyz: the camera moves along the x-y-z axes,
(3) rpy: the camera rotates over roll, pitch and yaw axes,
(4) static: the camera is kept static manually.

{% asset_img dynaslam-6.png 'DynaSLAM和其他语义或动态SLAM的对比' %}
上表是DynaSLAM和动态SLAM的效果对比, 可以看到DynaSLAM几乎在各个场景中都是力压群雄.
{% asset_img dynaslam-7.png '单目DynaSLAM和ORB-SLAM的对比' %}
上表是单目相机版的DynaSLAM和单目的ORB-SLAM的对比, 这一次DynaSLAM的精度不如ORB-SLAM, 论文的解释是DynaSLAM跟踪的路线比ORB-SLAM的更长, 所以累计误差比较多(反正我是不信). 这里我们还可以横向对比, 对比RGB-D的DynaSLAM和单目的DynaSLAM, 可以看到单目的DynaSLAM的误差少了那么一点点, 所以论文还做出了一个结论: 单目的SLAM更适合进行动态SLAM, 原因是单目和RGB-D的SLAM初始化方式不一样, 对于这一点, 我还没有找到根据, 但是从结果来看确实是这样.

## 2. CubeSLAM

[论文链接](https://arxiv.org/abs/1806.00557)
[参考翻译](https://mp.weixin.qq.com/s/wYhltIpqEFj7ubI12umukg)
这是一个物体级别的SLAM框架, 把物体考虑进SLAM的求解当中, 利用单视图也就是一帧, 进行三维立方体拟合.

### 2.1 三维立体表示

在该SLAM中, 3维物体都抽象成一个立方体, 用一个9维向量描述, 除了6维刚体变换外，还增加了3个元素: 即立方体的长, 宽, 高.
由于一个前端矩形检测的四个端点只能提供4个约束, 因而需要其他的信息. 论文中使用消失点(Vanishing point)VP来改变和减少回归参数
消失点是中心投影中的概念, 指三维空间中平行的直线在二维投影空间中的交点. 如下图中黑点
{% asset_img cubeslam.png '消失点' %}
3维立方体在二维平面中的投影大致可以分为三种情况, 3个面, 2个面, 一个面, 如下图:
{% asset_img cubeslam-1.png 'VP法示意图' %}
以a图举例, 如果我们知道2D框, 三个消失点的坐标和上方的一个立方体角点的投影, 如1, 那我们就可以推断出整个立方体的8个角点在二维平面上的投影. 得到二维点之后进行反投影就可以得到三维点的坐标了. 论文中说可以通过物体的旋转R和相机内参K得到三个消失点. 所以现在问题就变成了如何得到物体的旋转和一个角点. 论文中提到三种方法:

1. 利用深度学习网络进行大量的训练, 直接预测物体的旋转R(此方法后文不再提及)

1. 进行充分的采样, 评分选出最优的结果

1. 对于平放在地面上的物体, 利用RPY计算旋转(论文进行测试时所用的方法, 论文也只对于平放在地面上物体进行测试, 此时R=0, P=0, 只有Y角可以不为0)

对于第二个方法, 论文中给出了评分公式
{% asset_img cubeslam-2.png '评分公式' %}
其中, $\phi_{dist}$表示距离误差, 指提取出来的边缘和拟合的立方体的边之间的距离;

$\phi_{angle}$表示角度误差, 指消失点与角点的连线和检测出来的连线之间的误差;
{% asset_img cubeslam-4.png '角度误差计算公式' %}
$\phi_{shape}$表示形状误差, 将生成的2维点反投影到3维空间中, 计算不同边之间的长宽比, 加入惩罚, 这样可以筛除类似梯形的立方体;

示例效果如下图:
{% asset_img cubeslam-3.png '提案评分' %}

### 2.2 BA优化问题

总公式如下
{% asset_img cubeslam-5.png 'BA优化公式' %}

下面逐项解释

1. 相机-物体误差
    {% asset_img cubeslam-6.png '相机-物体误差公式' %}
    这个误差是表示成向量形式, 中间的空格是分开两个元素的, 我一开始一直看不懂这空格是干啥. 其中, $T_{om}$是当前帧预测的立方体坐标(包括位移和旋转6个量), $T_o$是SLAM建图中存储的该立方体的位姿, $T_c$是相机在世界坐标系中的位姿表示, 所以第一项的含义是预测的坐标和已有的坐标转换到相机坐标系下的坐标进行对比, 如果两个完全一样, 那么 $T^{-1}_c T_o T^{-1}_om$等于单位阵, 再通过se3转换为向量, 最后取对数; 矩阵的第二项就很简单, 是预测尺寸和已有尺寸的差

1. 相机-点误差
    论文中使用了标准的重投影误差

1. 物体-点误差
    {% asset_img cubeslam-8.png '物体-点误差' %}
    如果点有属于某一个物体, 那么点在此物体内时, 误差为0, 否则是点与该物体的距离.

### 2.3效果示例

{% asset_img cubeslam-9.png '效果示例' %}

## 3. 港科大沈劭劼团队

[论文链接](https://arxiv.org/abs/1807.02062)

提出了一种粗略估计汽车视角的算法, 该算法将汽车的视角粗略的分为8个视角, 利用Faster R-CNN, 在网络的最后添加一个和生成2d框并行的全连接网络, 输出汽车的视角.
{% asset_img shenshaojie.png '沈劭劼' %}

## 4. Network Uncertainty

[论文链接](https://arxiv.org/abs/1811.11946)

该论文使用信息熵判断观测数据是否用于更新估计的状态量.
对于统计变量X, 它的熵记为H(X), 在Y条件下记为H(X|Y), 两者的差值记为I(X;Y)
{% asset_img entropy-0.png '信息熵公式' %}
论文中推导出了如下公式:
{% asset_img entropy-1.png '论文推导的信息熵公式' %}
其中 $ H(x|Z) $ 表示在Z观测数据(一般指点)下, 位姿的不确定度, $ H(x|z_i, Z) $ 表示在加入了 $ z_i $ 观测点之后, x的不确定度变化, 加入语义信息之后, 还能得到如下公式:
{% asset_img entropy-2.png '语义信息熵公式' %}
其中, $ H(c_i|I,D) $ 表示, 在数据集D, 输入图像为I下, 分类信息 $ c_i $ 的不确定度, 也就是说, 在使用D数据集训练的网络, 输入I图像, 得到的分类结果$ c_i $ , 它在各个类的分类概率越分散, 熵越大, 分类概率越集中, 熵越小.
它的特征点选择策略就是, 对每一个点计算该值, 该值越大, 越容易被选择.

### 4.1 论文的意义

论文的意义就是可以在大量减少特征点的情况下, 保持跟原本差不多的精度.
{% asset_img entropy-3.png '效果对比' %}

## 5. 关键帧选取方法

[论文链接](https://ieeexplore.ieee.org/abstract/document/8793923)

1. 调整大小

1. 使用拉普拉斯算子卷积计算模糊度, 实际就是计算梯度, 梯度越大, 得分越高

1. 将图像投影到LAB色彩空间, 取L, 即亮度的值, 亮度越高, 得分越低

1. 使用MiniNet进行语义分割, 得到语义得分(但是我没有看到怎么由结果得到得分)

网络结构图如下
{% asset_img keyframe-1.png 'mininet网络结构图' %}

## 6. CALC

[论文链接](https://arxiv.org/abs/1805.07703)

一种使用无监督学习神经网络的回环检测方法, 设计了一种自动编码结构. 它是一种轻量级的实时快速学习架构.

{% asset_img calc-1.png 'calc结构图' %}
这是训练时的网络结构, 由两个卷积加池化层, 一个卷积层和三个全连接层组成, 训练完成之后保留黑框内的结构, 即一个输入层, 两个卷积加池化层和一个卷积层, 输出的就是图像的编码, 进行回环检测的时候直接对比图像的编码.
论文中还提到, 如果帧的数量比较多, 可以创建一个最近邻分类的KD树, 提高查询速度.

下面是不同数据集下查找结果对比
{% asset_img calc-2.png '结果1' %}
{% asset_img calc-3.png '结果2' %}
{% asset_img calc-4.png '结果3' %}
{% asset_img calc-5.png '结果4' %}
{% asset_img calc-6.png '结果5' %}

下面是提取特征描述子和查找最相近的帧的耗时(查找方式使用的线性搜索)
{% asset_img calc-7.png '速度对比' %}
